% extends "base.html" %}
{% block content %}
<div class="container">

  <!-- Title and Updated Subtitle -->
  <h1 class="main-title">🖼️ Image Bias Analyzer</h1>
  <p class="subtitle">
    You can upload your preprocessed CSV file here and get the analysis of the bias in your image datasets.
    Below we have mentioned the steps of preprocessing your dataset.
    <br><br>
    📌 We are currently working on providing a full pipeline for preprocessing — it’s still in progress!
  </p>

  <!-- Button to Import and Analyze -->
  <div style="margin-bottom: 40px;">
    <a href="{{ url_for('image_upload') }}">
      <button class="primary-btn">📁 Import CSV and Analyze</button>
    </a>
  </div>

  <!-- Metric Explanation -->
  <div class="card">
    <h2>📊 About the Metric (UBM - Unified Bias Metric)</h2>
    <p>This approach uses SHAP-based feature importance, PCA-derived spatial variance, and Ridge Regression to weight bias components.</p>
    <button onclick="toggleMethodology()" class="primary-btn">See Full Methodology</button>
    <div id="methodology-details" style="display: none;">
      <h3>🧠 Step-by-Step Calculation</h3>
      <ul>

        <li><strong>🔍 SHAP + PCA: Feature Weighting Strategy</strong>
          <p>
            We use SHAP (SHapley Additive exPlanations) to understand the importance of each feature in predicting gender, based on how much each feature contributes to a model’s decision.
            <br><br>
            PCA (Principal Component Analysis) is used alongside SHAP to capture the variance and structural patterns in the spatial features. By combining SHAP (60%) and PCA (40%) weights, we ensure that both interpretability and variance structure are reflected in the final bias metric.
          </p>
        </li>

        <li><strong>🧱 Object Influence Score (OIS):</strong><br>
          <code>OIS = w₁ × Relative_Size + w₂ × 3D_Distance + w₃ × Normalized_Depth</code>
          <p>
            OIS measures how visually prominent each object is in an image. We selected these three core features for their relevance:
          </p>
          <ul>
            <li><b>Relative_Size</b>: The proportion of image space occupied by the object — larger objects tend to attract more attention.</li>
            <li><b>3D_Distance</b>: Estimated spatial distance between the object and the camera — closer objects tend to be more visually dominant.</li>
            <li><b>Normalized_Depth</b>: Smoothed depth estimates from per-pixel depth maps, normalized across the dataset.</li>
          </ul>
          <p>
            The weights (w₁, w₂, w₃) are dynamically calculated based on SHAP + PCA results, adapting to the data distribution.
          </p>
        </li>

        <li><strong>🌄 Scene Similarity Bias (SSB):</strong><br>
          <code>SSB = CosineDistance(Scene_Embedding, V_Male) - CosineDistance(Scene_Embedding, V_Female)</code>
          <p>
            This metric evaluates whether a scene is semantically closer to male-associated or female-associated contexts.
            <br><br>
            We use <b>CLIP</b> and <b>Places365</b> models to generate scene-level embeddings. These embeddings are compared against predefined male and female vectors using cosine distance. The result reflects if a scene is contextually biased toward a gender.
          </p>
        </li>

        <li><strong>🧮 Unified Bias Score (UBM):</strong><br>
          <code>
            Bias_Score = [α × (OISₘ - OIS𝒇) + β × (SSBₘ - SSB𝒇)] / [α × (OISₘ + OIS𝒇) + β × (SSBₘ + SSB𝒇) + ε]
          </code>
          <p>
            This final score aggregates both OIS and SSB into a single contextual bias value. The formula compares how much visual influence and scene alignment differ between males and females for each object class.
            <br><br>
            The parameters <b>α</b> and <b>β</b> are learned using <b>Ridge Regression</b> — a regularized model that ensures balanced influence between OIS and SSB.
          </p>
        </li>

        <li><strong>📊 Bias Type Categorization:</strong>
          <ul>
            <li><b>Bias Score &gt; +Threshold</b> → <span style="color:blue"><b>Male-Biased 🔵</b></span></li>
            <li><b>Bias Score &lt; -Threshold</b> → <span style="color:red"><b>Female-Biased 🔴</b></span></li>
            <li><b>Else</b> → <b>Neutral ⚪</b></li>
          </ul>
          <p>
            Thresholds are dynamically computed based on the standard deviation of scores. This ensures consistency across datasets with different score distributions.
          </p>
        </li>

      </ul>
    </div>

  </div>

  <!-- Preprocessing Steps -->
  <div class="card">
    <h2>⚙️ Preprocessing Instructions</h2>
    <p>We use the following pre-trained models to extract contextual features:</p>
    <ul>
      <li><b>YOLOv8</b>: Detects objects and persons, returns bounding boxes and labels.</li>
      <li><b>SAM (Segment Anything Model)</b>: Improves object masks for better size estimates.</li>
      <li><b>DepthAnything v2</b>: Generates per-pixel depth maps to compute 3D spatial distances.</li>
      <li><b>CLIP & Places365</b>: Extract scene embeddings to compute semantic alignment with gendered environments.</li>
    </ul>
    <p>These models provide the features used in OIS and SSB. Your CSV must contain:</p>
    <code>Object_Class, Gender, Relative_Size, 3D_Distance, Normalized_Depth, Scene_Similarity_Bias</code>
    <pre><code>
# Sample Python Snippet
import pandas as pd
df = pd.read_csv("your_data.csv")
required_cols = ["Object_Class", "Gender", "Relative_Size", "3D_Distance", "Normalized_Depth", "Scene_Similarity_Bias"]
df = df[required_cols]
    </code></pre>
  </div>
</div>

<!-- Toggle Methodology JS -->
<script>
  function toggleMethodology() {
    var section = document.getElementById("methodology-details");
    section.style.display = section.style.display === "none" ? "block" : "none";
  }
</script>

<!-- Styling -->
<style>
  .container {
    padding: 30px;
  }
  .main-title {
    font-size: 30px;
    color: #333;
    margin-bottom: 10px;
  }
  .subtitle {
    font-size: 16px;
    margin-bottom: 25px;
  }
  .card {
    background: #ffffff;
    padding: 20px;
    border-left: 6px solid #ffb291;
    border-radius: 8px;
    margin-bottom: 40px;
  }
  .primary-btn {
    background-color: #333;
    color: white;
    padding: 10px 20px;
    border-radius: 8px;
    font-weight: bold;
    border: none;
    cursor: pointer;
  }
  .primary-btn:hover {
    background-color: #5f5e5e;
  }
  .code-box, pre code {
    background-color: #f4f4f4;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    display: block;
  }
</style>
{% endblock %}